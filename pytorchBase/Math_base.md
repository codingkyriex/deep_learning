# 数学基础
## 1.softmax：
### **交叉熵**
**交叉熵主要是用来判定实际出与期望的输出的接近程度**，为什么这么说呢，举个例子：在做分类的训练的时候，如果一个样本属于第K类，那么这个类别所对应的的输出节点的输出值应该为1，而其他节点的输出都为0，即[0,0,1,0,….0,0]，这个数组也就是样本的Label，是神经网络最期望的输出结果。也就是说用它来衡量网络的输出与标签的差异，利用这种差异经过反向传播去更新网络参数。
### **信息量**
它是用来衡量一个事件的不确定性的；一个事件发生的概率越大，不确定性越小，则它所携带的信息量就越小。假设X是一个离散型随机变量，其取值集合为X，概率分布函数为 **p(x)=P(X=x),x $\in$ X** ，我们定义事件**X=x0**的信息量为
**I(x0)=- $\log$ (p(x0))**
当 **p(x0)=1**时，熵将等于0，也就是说该事件的发生不会导致任何信息量的增加。
### **熵**
它是用来衡量**一个系统的混乱程度**的，代表一个系统中信息量的总和；信息量总和越大，表明这个系统不确定性就越大。
### **交叉熵**
它主要刻画的是**实际输出（概率）与期望输出（概率）的距离**，也就是交叉熵的值越小，两个概率分布就越接近。假设概率分布p为期望输出，概率分布q为实际输出$H(p,q)$为交叉熵，则
$$
H(p,q)=-\sum(p(x)logq(x)+(1-p(x)log(1-q(x))))
$$
