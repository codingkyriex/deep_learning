{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、线性回归实现(纯手撸)\n",
    "## 1.生成数据集\n",
    "### 使用函数部分\n",
    "* **torch.normal**：返回一个张量，包含从给定参数means,std的*离散正态分布*中抽取随机数。 均值means是一个张量，包含每个输出元素相关的正态分布的均值。 std是一个张量，包含每个输出元素相关的正态分布的标准差。 均值和标准差的形状不须匹配，但每个张量的元素个数须相同。同时out可以指定输出张量的形状（shape）\n",
    "* **torch.matmul**: 二维时等同于矩阵乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "def synthetic_data(w, b, num_examples):  #@save\n",
    "    \"\"\"生成y=Xw+b+噪声\"\"\"\n",
    "    X = torch.normal(0, 1, (num_examples, len(w)))\n",
    "    y = torch.matmul(X, w) + b\n",
    "    y += torch.normal(0, 0.01, y.shape)# 加上噪音\n",
    "    return X, y.reshape((-1, 1))\n",
    "\n",
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)\n",
    "features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.生成数据集图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig=plt.figure()\n",
    "ax=fig.add_axes([0,0,1,1])\n",
    "ax.scatter(features[:,0],labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.读取数据集\n",
    "### 使用函数部分\n",
    "* **random.shuffle()**: 用于将一个列表中的元素打乱顺序，值得注意的是使用这个方法不会生成新的列表，只是将原列表的次序打乱。\n",
    "* **yield**:带yield的函数是一个<mark>生成器</mark>，而不是一个函数了，这个生成器有一个函数就是next函数，next就相当于“下一步”生成哪个数，这一次的next开始的地方是接着上一次的next停止的地方执行的，所以调用next的时候，生成器并不会从foo函数的开始执行，只是接着上一步停止的地方开始，然后遇到yield后，return出要生成的数，此步就结束。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 小批量随机读取\n",
    "import random\n",
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    # indices代表一个随机序号列表\n",
    "    # 这些样本是随机读取的，没有特定的顺序\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        # 每次取出indices中的batch个序列表\n",
    "        batch_indices = torch.tensor(\n",
    "            indices[i: min(i + batch_size, num_examples)])\n",
    "        yield features[batch_indices], labels[batch_indices]\n",
    "batch_size = 10\n",
    "for X, y in data_iter(batch_size, features, labels):\n",
    "    print(X, '\\n', y)\n",
    "    #只取一组\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg(X, w, b):  #@save\n",
    "    \"\"\"线性回归模型\"\"\"\n",
    "    return torch.matmul(X, w) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.定义损失算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_hat, y):  #@save\n",
    "    \"\"\"均方损失\"\"\"\n",
    "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.定义优化算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size):  #@save\n",
    "    \"\"\"小批量随机梯度下降\"\"\"\n",
    "    with torch.no_grad():#异常处理\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size#规范化步长\n",
    "            param.grad.zero_()#梯度归零"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import requires\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "def synthetic_data(w, b, num_examples):  #@save\n",
    "    \"\"\"生成y=Xw+b+噪声\"\"\"\n",
    "    X = torch.normal(0, 1, (num_examples, len(w)))\n",
    "    y = torch.matmul(X, w) + b\n",
    "    y += torch.normal(0, 0.01, y.shape)# 加上噪音\n",
    "    return X, y.reshape((-1, 1))\n",
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    # indices代表一个随机序号列表\n",
    "    # 这些样本是随机读取的，没有特定的顺序\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        # 每次取出indices中的batch个序列表\n",
    "        batch_indices = torch.tensor(\n",
    "            indices[i: min(i + batch_size, num_examples)])\n",
    "        yield features[batch_indices], labels[batch_indices]\n",
    "def linreg(X, w, b):  #@save\n",
    "    \"\"\"线性回归模型\"\"\"\n",
    "    return torch.matmul(X, w) + b\n",
    "def squared_loss(y_hat, y):  #@save\n",
    "    \"\"\"均方损失\"\"\"\n",
    "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2\n",
    "def sgd(params, lr, batch_size):  #@save\n",
    "    \"\"\"小批量随机梯度下降\"\"\"\n",
    "    with torch.no_grad():#异常处理\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size#规范化步长\n",
    "            param.grad.zero_()#梯度归零\n",
    "\n",
    "batch_size = 10\n",
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "w=torch.tensor([1.,-3.])\n",
    "b=4.\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)\n",
    "lr = 0.03\n",
    "num_epochs = 3 # 迭代周期个数\n",
    "net = linreg\n",
    "loss = squared_loss\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        l = loss(net(X, w, b), y)  # X和y的小批量损失\n",
    "        # 因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起，\n",
    "        # 并以此计算关于[w,b]的梯度\n",
    "        l.sum().backward()\n",
    "        sgd([w, b], lr, batch_size)  # 使用参数的梯度更新参数\n",
    "    with torch.no_grad():\n",
    "        train_l = loss(net(features,w, b), labels)\n",
    "        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、深度学习框架解决\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成数据集与上图类似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\coding_setting\\Miniconda\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "def synthetic_data(w, b, num_examples):  #@save\n",
    "    \"\"\"生成y=Xw+b+噪声\"\"\"\n",
    "    X = torch.normal(0, 1, (num_examples, len(w)))\n",
    "    y = torch.matmul(X, w) + b\n",
    "    y += torch.normal(0, 0.01, y.shape)# 加上噪音\n",
    "    return X, y.reshape((-1, 1))\n",
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据集--使用深度学习框架\n",
    "### 使用函数部分\n",
    "* data.TensorDataset():TensorDataset 可以用来对 tensor 进行打包，就好像 python 中的 zip 功能。该类通过每一个 tensor 的第一个维度进行索引。因此，该类中的 tensor 第一维度必须相等. 另外：TensorDataset 中的参数必须是 tensor\n",
    "* data.DataLoader就是用来包装所使用的数据，每次抛出批量大小为batch_size的数据，可选择是否随机打乱，它是一个迭代器\n",
    "* nn:神经网络缩写\n",
    "* Sequential:神经网络的构造容器，可以理解为放置多个层的大容器，当给定输入数据时，Sequential实例将数据传入到第一层， 然后将第一层的输出作为第二层的输入，以此类推。\n",
    "* Linear：torch.nn.Linear(in_features, *输入的神经元个数* out_features, *输出神经元个数*\n",
    "           bias=True  *是否包含偏置*)，默认会利用标准正态分布初始化w和b，也可以自己定义初始值。\n",
    "* MSELoss: 返回样本损失平均值\n",
    "* torch.optim.SGD: 小批量随机梯度下降算法是一种优化神经网络的标准工具， PyTorch在optim模块中实现了该算法的许多变种。 当我们实例化一个SGD实例时，我们要指定优化的参数 （可通过net.parameters()从我们的模型中获得）以及优化算法所需的超参数字典。 小批量随机梯度下降只需要设置lr值(学习率)，这里设置为0.03。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-6.0436e-01,  4.2205e-02],\n",
       "         [-3.3791e-01, -1.7858e-01],\n",
       "         [ 1.3887e+00, -1.0825e+00],\n",
       "         [ 9.7460e-01,  3.4841e-01],\n",
       "         [-1.7414e-03,  3.9982e-01],\n",
       "         [ 7.7913e-01, -3.9115e-01],\n",
       "         [ 1.9110e+00, -3.2618e-01],\n",
       "         [ 1.1407e+00, -2.9752e-01],\n",
       "         [ 5.4157e-01,  5.8803e-01],\n",
       "         [ 1.8378e+00, -1.7873e-01]]),\n",
       " tensor([[ 2.8587],\n",
       "         [ 4.1368],\n",
       "         [10.6487],\n",
       "         [ 4.9791],\n",
       "         [ 2.8637],\n",
       "         [ 7.0934],\n",
       "         [ 9.1467],\n",
       "         [ 7.4832],\n",
       "         [ 3.2930],\n",
       "         [ 8.4818]])]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_array(data_arrays, batch_size, is_train=True):  #@save\n",
    "    \"\"\"构造一个PyTorch数据迭代器\"\"\"\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "batch_size = 10\n",
    "data_iter = load_array((features, labels), batch_size)\n",
    "\n",
    "next(iter(data_iter))#迭代器执行一次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn是神经网络的缩写\n",
    "from torch import nn\n",
    "net = nn.Sequential(nn.Linear(2, 1))# 输入特征数为2，输出特征数为1\n",
    "# net[0].weight.data.normal_(0,0.01)# 训练时的初始权重赋值\n",
    "# net[0].bias.data.fill_(0)# 训练时的初始偏差赋值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = torch.optim.SGD(net.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.000238\n",
      "epoch 2, loss 0.000103\n",
      "epoch 3, loss 0.000104\n"
     ]
    }
   ],
   "source": [
    "#开始训练\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter: #取出数据集中的小批量数据\n",
    "        l = loss(net(X) ,y) # 计算单次损失率\n",
    "        trainer.zero_grad()# 梯度归零\n",
    "        l.backward()# 计算梯度\n",
    "        trainer.step()# 进行参数的优化\n",
    "    l = loss(net(features), labels)# 一次优化结束后的损失率情况\n",
    "    print(f'epoch {epoch + 1}, loss {l:f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w的估计误差： tensor([[-0.0007,  0.0010]])\n",
      "b的估计误差： tensor([4.7684e-06])\n"
     ]
    }
   ],
   "source": [
    "# 三次优化完计算误差\n",
    "w = net[0].weight.data\n",
    "print('w的估计误差：', true_w - w.reshape(true_w.shape))\n",
    "b = net[0].bias.data\n",
    "print('b的估计误差：', true_b - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总结\n",
    "## 线性回归的步骤总结\n",
    "* 1.建立数据集（一般来说读取给定的csv文件）\n",
    "* 2.读取数据集，使用TensorDataset打包，再使用DataLoader读取批量的数据。\n",
    "* 3.定义神经网络，将每一层封装在容器Sequential中。\n",
    "* 4.定义损失函数\n",
    "* 5.定义训练模型并初始化\n",
    "* 6.进行训练"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "462caf5131bdaea9f40f72fd77030f3452d221c2a6e1b91cdb9218e97e705d92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
