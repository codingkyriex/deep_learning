{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多层感知机\n",
    "## 一、使用函数\n",
    "* nn.Parameter():torch.nn.Parameter()将一个不可训练的tensor转换成可以训练的类型parameter，并将这个parameter绑定到这个module里面。即在定义网络时这个tensor就是一个可以训练的参数了。使用这个函数的目的也是想让某些变量在学习的过程中不断的修改其值以达到最优化。换句话说，在不使用神经网络层时它的作用是形成可更新的张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、从零开始实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\coding_setting\\Miniconda\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "# 返回批量batch_size的数据\n",
    "def load_data_fashion_mnist(batch_size, resize=None):  #@save\n",
    "    \"\"\"下载Fashion-MNIST数据集，然后将其加载到内存中\"\"\"\n",
    "    trans = [transforms.ToTensor()]# 一个函数的列表\n",
    "    if resize:\n",
    "        trans.insert(0, transforms.Resize(resize))# 函数功能增加一项\n",
    "    trans = transforms.Compose(trans)# 组合图像操作\n",
    "    # 与上面类似，下载图像集\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(\n",
    "        root=\"../data\", train=True, transform=trans, download=True)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(\n",
    "        root=\"../data\", train=False, transform=trans, download=True)\n",
    "    return (data.DataLoader(mnist_train, batch_size, shuffle=True,\n",
    "                        ),# 使用几个进程进行加载，默认为0\n",
    "            data.DataLoader(mnist_test, batch_size, shuffle=False,# 代表主线程加载\n",
    "                            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "train_iter,test_iter=load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 推导过程：\n",
    "> Fashion-MNIST中的每个图像由$28*28=784$个灰度像素值组成。所有图像共分为10个类别，也就是说输入特征数为784，最后的输出特征为10，也就是说输入层为$x \\in R^{256\\times784}$, 隐藏层的隐藏个数为256个，也就是说明$w1 \\in R^{784\\times256}$$b1 \\in R^{1\\times256}$\n",
    "> 输出层为10个特征，则$w2 \\in R^{256\\times10}$,$b2 \\in R^{1\\times10}$\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens = 784, 10, 256\n",
    "\n",
    "W1 = nn.Parameter(torch.randn(\n",
    "    num_inputs, num_hiddens, requires_grad=True) * 0.01)\n",
    "b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))\n",
    "W2 = nn.Parameter(torch.randn(\n",
    "    num_hiddens, num_outputs, requires_grad=True) * 0.01)\n",
    "b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))\n",
    "\n",
    "params = [W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只保留正数的激活函数\n",
    "def relu(X):\n",
    "    a = torch.zeros_like(X)\n",
    "    return torch.max(X, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    X = X.reshape((-1, num_inputs))\n",
    "    H = relu(X@W1 + b1)  # 这里“@”代表矩阵乘法\n",
    "    return (H@W2 + b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用softmax实现中使用的softmax函数\n",
    "loss = nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):  #@save\n",
    "    \"\"\"计算预测正确率\"\"\"\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "    cmp = y_hat.type(y.dtype) == y\n",
    "    return float(cmp.type(y.dtype).sum())/y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net,num_epochs,train_iter,test_iter,loss,trainer):\n",
    "    all_accuracy=0.0\n",
    "    for i in range(num_epochs):\n",
    "        for X,y in train_iter:\n",
    "            y_hat=net(X)\n",
    "            l=loss(y_hat,y)\n",
    "            trainer.zero_grad()\n",
    "            l.mean().backward()\n",
    "            trainer.step()\n",
    "        one_accuracy=accuracy(y_hat,y)\n",
    "        all_accuracy+=one_accuracy\n",
    "        print(f'第{i+1}次测试,损失率为{l.mean()},准确率为{one_accuracy}')\n",
    "    print(f'平均准确率为{all_accuracy/num_epochs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1次测试,损失率为0.5537840723991394,准确率为0.7916666666666666\n",
      "第2次测试,损失率为0.49689531326293945,准确率为0.8229166666666666\n",
      "第3次测试,损失率为0.3481275141239166,准确率为0.9270833333333334\n",
      "第4次测试,损失率为0.3859090507030487,准确率为0.8645833333333334\n",
      "第5次测试,损失率为0.44069480895996094,准确率为0.84375\n",
      "第6次测试,损失率为0.36168432235717773,准确率为0.8958333333333334\n",
      "第7次测试,损失率为0.4410228729248047,准确率为0.84375\n",
      "第8次测试,损失率为0.3754465579986572,准确率为0.8645833333333334\n",
      "第9次测试,损失率为0.3595719337463379,准确率为0.8854166666666666\n",
      "第10次测试,损失率为0.35955002903938293,准确率为0.90625\n",
      "平均准确率为0.8645833333333333\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr = 10, 0.1\n",
    "updater = torch.optim.SGD(params, lr=lr)\n",
    "train(net, num_epochs,train_iter, test_iter, loss, updater)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、使用深度学习框架"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Flatten(),\n",
    "                    nn.Linear(784, 256),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(256, 10))\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std=0.01)\n",
    "\n",
    "net.apply(init_weights);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1次测试,损失率为0.5564848184585571,准确率为0.8020833333333334\n",
      "第2次测试,损失率为0.5153186321258545,准确率为0.8645833333333334\n",
      "第3次测试,损失率为0.483040452003479,准确率为0.7916666666666666\n",
      "第4次测试,损失率为0.4980808198451996,准确率为0.8333333333333334\n",
      "第5次测试,损失率为0.5532759428024292,准确率为0.8020833333333334\n",
      "第6次测试,损失率为0.45694032311439514,准确率为0.84375\n",
      "第7次测试,损失率为0.607175886631012,准确率为0.84375\n",
      "第8次测试,损失率为0.4522967040538788,准确率为0.8229166666666666\n",
      "第9次测试,损失率为0.40743720531463623,准确率为0.8229166666666666\n",
      "第10次测试,损失率为0.3081527650356293,准确率为0.8854166666666666\n",
      "平均准确率为0.83125\n"
     ]
    }
   ],
   "source": [
    "batch_size, lr, num_epochs = 256, 0.1, 10\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size)\n",
    "train(net, num_epochs,train_iter, test_iter, loss,  trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、总结\n",
    "* 多层感知机的核心就是采用一层或多层的隐藏层执行非线性仿射变化，来消除单一线性变化带来的误差"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "462caf5131bdaea9f40f72fd77030f3452d221c2a6e1b91cdb9218e97e705d92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
